\chapter{Compressed Sensing}\label{C:compressed_sensing}

%%re-wr
Compressed sensing is a novel research area introduced by Donoho \cite{donoho2006sparse} and by Candes, Romberg, and Tao \cite{candes2006robust} in 2006, and since then has already become a key concept in various areas of applied mathematics, computer science, and electrical engineering. It surprisingly predicts that high-dimensional signals, which allow a sparse representation by a suitable basis or, more generally, a frame, can be recovered from what was previously considered highly incomplete linear measurements by using efficient algorithms. This chapter shows an introduction to and a survey of compressed sensing.
%%

\section{Introduction}\label{ch1:intro}

In the real world, people often observe and save signals in digital devices for further analysis, e.g., medical image imaging, CT tomography. Using the discrete sampling and storage, the system are prospected to exactly reconstruct the original signals. If we assume that original signal is $1 \times N$ vector, the observation behaviour can be modelled as a sensing matrix A with a size of $m \times N$. Then the sampled data $y$ can be described as
\begin{equation}
\label{sampling-eq}
y = Ax,
\end{equation}
Then an important question emerges:
\begin{itemize}
\item In order to reconstruct signal $x$, \emph{at least} how many samples $y$ are needed?
\end{itemize}
Traditional linear algebra theory states that unique solution of (\ref{sampling-eq}) exists if $m = N$. That is to say, we need at least N samples $y$.
However, consider a case when the signal $x$ is sparse, which indicates most of values in $x$ are zero or relatively small, is there any method to reconstruct $x$ using smaller numbers of observations $y$? ($m < N$)

Conventionally, if $m < N$, the the equation (\ref{sampling-eq}) is underdetermined, so various solutions exist which means it cannot uniquely recover the original signal $x$. However, if the $x$ is sparse, then the answer maybe \emph{Yes}: Using the theory of the compressed sensing (CS), surprisingly it allows a possible sparse information's reconstruction from a very few numbers of samples $y$ ($m << N$), where the sparse information can naturally exist (such as $x$) or generated by orthogonal basis decomposition (such as $x = As$ where $s$ is sparse, $A$ is the basis e.g. wavelets decomposition). Consequently, the CS provides an possible method to reduce the number of observations theoretically. In conclusion, the task of CS can be abstracted as:
\begin{itemize}
\item Find the minimum number of observations for the sensing matrix.
\item Design reconstruction algorithm to uniquely recover original signals via those observations.
\end{itemize}

\section{Sparsity: A Reasonable Assumption}

\subsection{Definition of sparse signals}

To state the problem mathematically precisely, let now $x \in R^{N \times 1}$ be our signal of interest. As prior information, we either assume that $x$ itself is sparse, i.e., it has very few $K$ non-zero coefficients in the sense that
\begin{equation}
\label{sparse_eq}
\| x \|_0 := \{i: x_i \neq 0\}
\end{equation}
is small, or that there exists an orthonormal basis or a frame $\Psi$ such that $x = \Psi s$, where $x$ is a linear combination of few $k$ basis chosen from $\Psi$, and $s$ is the corresponding coefficients of representing $x$ in the domain constructed by the basis $\Psi$, $k$ is termed as the sparsity.

%%re-wr
For real world signals, a variety of representation systems can be used to provide sparse approximations. If this is not the case, more data sensitive methods such as dictionary learning algorithms, in which a suitable representation system is computed for a given set of test signals, are available This process is customarily termed dictionary learning. The most well-known adaptive learning algorithm is the K-SVD algorithm introduced by Aahron, Elad, and Bruckstein in \cite{aharon2006img}. In this chapter, we will assume throughout that $x$ is already a sparse vector or can be sparse represented.
%%

\section{Model of Sparse Reconstruction}

When the $x$ is sparse, an intuitive solution of the underdetermined equation (\ref{sampling-eq}), equals the solution of the problem (P0), which aims to find the minimum numbers of supports in $x$ as following:
\begin{equation}
\label{eq_l0}
P0: \quad min \| x \|_0 \quad s.t. \quad y = Ax
\end{equation}
Then the following Theorem presents the sufficient condition to gain the unique solution:
\begin{theorem}
\label{eq-P0}
Assume that $A \in R^{m \times N}$ is an average of the independent $2k$ column matrix. Then for any $k$-sparse vector $x$, the $x$ can be exactly recovered by solving the problem P0 (\ref{eq_l0}).
\end{theorem}
Hence, according to Theorem \ref{eq-P0}, then there is a coding and decoding of $(A, \Delta)$ such that $x = \Delta(Ax)$, which means $2k$ times the number of observations is sufficient for reconstruction. However, solving the problem P0 is very difficult. In fact, P0 is an NP-complete problem \cite{davis1997adaptive}.

So could we find a more efficient decoding algorithms? The answer maybe \emph{Yes} if some condition is satisfied. In this case, using the solution of the following problem P1, which is termed as Basis Pursuit (BP) or l1-norm minimisation, we can uniquely reconstruct $x$.
\begin{equation}
\label{eq_l1}
P1: \quad min \| x \|_1 \quad s.t. \quad y = Ax
\end{equation}

For solving the problem P1, various types of recovery algorithms exist, which can be sorted as convex optimisation, greedy, and combinatorial algorithms (section \ref{sct:recon-alg}), and each one having its own advantages and disadvantages. Here in this section, we provide the widely used linear programming (LP) as an example to solve P1:
\begin{equation}
\begin{split}
LP: \quad min \| t_1 + t_2 + \cdots + t_N \|_1 \quad t \in R^N\quad s.t. \quad y = Ax, \\
\quad -t_j \leq x \leq t_j, \quad t_j \geq 0, \quad j = 1 \cdots N
\end{split}
\end{equation}

\subsection{Null Space Property}

According to (\ref{eq_l1}), the sparse reconstruction is achievable when P1 equals P0. So letâ€™s return to the question: when P1 equals P0? In order to develop this question, the null space of sensing matrix plays a important role, which is defined as $N(A) = \{x: Ax = 0 \}$. Then the null space property and its corresponding theorem are given to answer that question, and presents us the sufficient and necessary condition for P1 = P0:
\begin{Def}
The $m \times N$ sensing matrix A satisfies the null space property of order k provided for every index set S with supp(S) = k, we have
\begin{equation}
\label{nsp-eq}
\| h_S \|_1 < \|h_{S^c}\|_1, \quad \forall h \in N(A), \quad h \neq \Theta
\end{equation}
\end{Def}
\begin{theorem}
\label{nsp-thm}
Every k-sparse vector $\hat x$ is the unique solution of the P1 problem (\ref{eq_l1}) if and only if A satisfies the null space property of order k \cite{devore2007deterministic}.
\end{theorem}
%re-wr
According to the Theorem \ref{nsp-thm}, the equivalent condition for the existence of a unique sparse solution of (P1) can now be stated in terms of the null space property (NSP). Then many convex optimisation algorithm and greedy method can be applied to solve P1 problem (section \ref{sct:convex}).

\subsection{Restricted Isometry Property}\label{sct:rip}

Although the NSP and the Theorem \ref{nsp-thm} presents us an equivalent condition to get unique reconstruction through solving P1, however, how to verify the sensing matrix satisfying NSP in (\ref{nsp-eq}) is computational complex, so NSP is not practically useful.

%re-wr
Then a more intuitive concept termed as Uniform Uncertainty Principle or Restricted Isometry Property (RIP) is introduced in \cite{candes2006robust}. It measures the degree to which each subset of k column vectors of the sensing matrix A is close to being an isometry.
\begin{Def}
\label{rip-def}
Let A be an $m \times n$ sensing matrix. Then A has the Restricted Isometry Property (RIP) of order k, if there exists a $\delta_k \in (0, 1)$ such that
\begin{equation}
(1-\delta_k)\|x\|_2 \leq \|Ax\|_2 \leq (1+\delta_k)\|x\|_2 \quad for \; all \; k\; sparse \; x
\end{equation}
\end{Def}
\begin{theorem}
\label{nsp-rip}
A sufficient condition for the null space property (NSP) to hold and for all k-sparse signal is that RIP holds for 2k-sparse signals with
\begin{equation}
\rho = \frac{\sqrt{2} \delta_{2k}}{1-\delta_{2k}} \leq 1
\end{equation}
It follows that if $\delta_{2k} \leq \sqrt{2}-1$ then NSP is satisfied, so that the unique reconstruction can be achieved by l1-norm minimisation (\ref{eq_l1}).
\end{theorem}
According to the Theorem \ref{nsp-rip}, an equivalent condition for the existence of a unique sparse solution of (P1) can now be stated in terms of RIP. If the sensing matrix satisfies the RIP, then for any k-sparse vector $x$, the Euclidian distance of projected x ( i.e. Ax ) and x still keep very close if the $\delta_{2k} \in (0,1)$ is relatively small.

\subsection{Robust Recovery}

Back to our original task in compressed sensing framework in section \ref{ch1:intro}, which aims at designing a efficient coding and decoding procedure, we now can use RIP-satisfied matrix for encoding, and l1-norm minimisation as decoding to recover k-sparse signals. However, in the real world, signals sometimes are not strictly sparse signals, but usually approximate sparse.
For such signals, is the framework of coding (RIP matrix projection) and decoding (l1-norm minimisation) still suitable? Interestingly, the answer is yes, it still can be better done: First, we define the concept of the best k-term approximation error:
\begin{equation}
\sigma_k(x)_p := min \| x - \hat x \|_p
\end{equation}
Then, the error boundary can be presented in terms of the best k-term approximation as follows:
\begin{theorem}
(\cite{candes2006robust}) Let A be an $m \times N $ matrix which satisfies the RIP of order 2k with $\delta_{2k} \leq \sqrt{2}-1$. Let $x \in RN$, and let $\hat x$ be a solution of the associated (P1) problem. Then
\begin{equation}
\label{robust-l1}
\| x-\hat x \|_2 \leq C \cdot ( \frac{{\sigma_k(x)}_1}{\sqrt k} )
\end{equation}
\end{theorem}

\subsection{Minimum Samples}

Letâ€™s return to the question in section \ref{ch1:intro}: In order to reconstruct signal $x$, \emph{at least} how many samples $y$ are needed? The \cite{gribonval2003sparse} shows the following Theorem which presents a lower boundary:
\begin{theorem}
\label{min-sample}
Assume a matrix $\in R^{m \times N}$ for sensing and l1-norm minimisation for reconstruction, then for any 2k-sparse vector x $\in R^{N}$, then minimum required number of observations is:
\begin{equation}
n \geq c_1 k log(\frac{N}{c_2 k}), \quad c_1 = 1/log 9 \; and \; c_2 = 4
\end{equation}
\end{theorem}
Following this Theorem, any underdetermined sensing matrix requires at least $O(k log(\frac{N}{k}))$ samples for sparse reconstruction in (\ref{eq_l1}). Then a new question emerges: Since the Theorem \ref{nsp-rip} presents a unique sparse solution of (P1) stated in terms of RIP, can this sensing and recovery approach (RIP-matrix sensing and l1-norm minimisation) reach the lower boundary of the minimum required samples that is $O(k log(\frac{N}{k}))$? The following section answers this question with respect to various of sensing matrix.

\section{Sensing Matrix}

Verification for RIP is not practical, for the RIP serves for every possible sparse vector, and \cite{bandeira2012certifying} demonstrates that testing whether a matrix satisfies RIP is NP-hard. However, fortunately, we can often construct the matrix satisfying RIP by using randomness, which gives a practical way for creating the sensing matrix.

\subsection{Random Matrix}

We consider two types of random matrices: Gaussian random matrix and Bernoulli random matrix. The Gaussian random matrices refers to the matrix whose elements $a_{i, j}$ are independent random variables and subject to the distribution $N(0,1)$. The Bernoulli matrix refers to the matrix whose elements $a_{i, j}$ has the same probability to be 1 or -1.

\subsubsection{Probability Reconstruction}

\begin{theorem}
\label{prob-recon}
Assumed $m \times N$ matrix A is a Gaussian or Bernoulli random matrix. Then, if $k \leq C_1 / log (N / k)$, matrix A satisfy the k-order RIP (\ref{rip-def}) with the probability bigger than
\begin{equation}
1 - exp (- C_2 n)
\end{equation}
, where constants $C_1$, $C_2$ depends only on the RIP constant $\delta$.
\end{theorem}
The Theorem shows that the Gaussian or Bernoulli random matrices satisfies the $k = O(n/ log(N/k)) $ order RIP, indicating that these random matrices reach the lower boundary of the minimum required samples that is $O(k log(\frac{N}{k}))$ in (\ref{min-sample}). Therefore, these random matrices presents an efficient and optimal coding and decoding method, which involve random sensing matrix projection and l1-norm minimisation respectively, and apply the minimum amount of observations.

\subsection{Deterministic Matrix}

Although a random matrix (Gaussian or Bernoulli) has high probability to satisfies RIP and provide optimal coding and decoding method in terms of number of samples, in practice, it is more desirable to construct a deterministic matrix because the certainty in matrix is more attractive. For example, from the perspective of structural decoding algorithms, deterministic matrix helps to reduce memory cost and improve reconstruction speed. The current construction methods for deterministic matrix are mainly based on the property of matrix coherence:
\begin{Def}
\label{matrix-coherence}
Assume a $m \times N$ sensing matrix A with normalised columns $A_1, \cdot, A _N$. The coherence of matrix A is defined as
\begin{equation}
\mu(A) = max_{i \neq j} | < A_i, A_j > |
\end{equation}
Then the lower bound of $\mu(A)$, termed as Welch bound, is given in \cite{welch1974lower} as:
\begin{equation}
\mu(A) \geq \sqrt{\frac{N-n}{(n-1)N}}
\end{equation}
\end{Def}
\begin{theorem}
\label{cohere-mu}
Assume matrix A has the coherence $\mu(A)$, then A satisfies $\delta_k = (k-1) \mu(A)$ order RIP \cite{weil1948some}.
\end{theorem}
Combining the Theorem \ref{cohere-mu} with the Welch bound in definition \ref{matrix-coherence}, the sensing matrix A must satisfy the $k = \sqrt{2n}$ RIP. Hence, if we only investigate matrix coherence, we hardly get the lower boundary of the minimum required samples $O(k log(\frac{N}{k}))$ in (\ref{min-sample}). Thus, how to construct the deterministic RIP matrix to match this lower bound of required samples is still an open issue \cite{xu2011deterministic}.

\subsection{Partial Random Structural Matrix}

Since there exists problem in constructing a deterministic matrix, and since there is always not enough freedom for building random Gaussian matrix subject to physical or other constraints in applications, at the moment, a new type of matrix structure is investigate as it has a combination features of partial randomness and partial deterministic design, which is termed as the partial random structural matrix. It becomes popular since most applications have constraints in design sensing matrix, although many those matrix cannot reach the optimal boundary of minimum required samples $O(k log(\frac{N}{k}))$ in (\ref{min-sample}).

\subsubsection{Random Partial Fourier Matrix}

When we randomly choose m rows in the Fourier matrix $\Psi$ to generate a $m \times N$ matrix, the matrix becomes the random partial Fourier matrix $\Psi_m$. Part of random Fourier matrix has a strong application background. For example, the randomly observed partial frequency information can be fully recovered by l1-norm minimisation \cite{candes2006near}. And according to the CS theory which has a $m \times N$ sensing matrix where $m << N$, the sampled points $m$ is far less than original required number $N$ under Nyquist rate. In addition, we just apply the random sample behaviour so that the low-rate CS sampling is achieved. In \cite{rudelson2008sparse}, the number of minimum samples is proven as $n = O (k (log N)^4)$.

\subsubsection{Rademacher Matrix}

The Rademacher random variable takes only values -1 and +1 with the same likelihood. In hardware design, it can be easily implemented, so the \cite{tropp2010beyond} use this sequence in a pre-mixer for random projection, and successfully designs a low rate analog to digital converter based on the CS framework, and terms this architecture as the random demodulator (details in the next chapter). From the theoretical aspect, the required minimum samples $m = O(k log^6 N)$. In practise, however, simulations show that the random demodulator requires just $O(klog(N/k))$ samples to stably reconstruct the signal. This result reaches the optimal boundary of minimum required samples $O(k log(\frac{N}{k}))$ in (\ref{min-sample}).

\subsubsection{Random Circulant Matrix}

In many cases where signal convolution is involved, the circulant matrix always exists whose each row is shifted compared to its neighbours. This is because that any n-point
circular convolution of two sequences $x \in R^N$ and $h \in R^N$ is $h \otimes x = Hx$, where H is the corresponding $N \times N$ circulant matrix to original impulse response h. In CS framework, the randomness is embedded into each elements of the matrix, e.g. Rademacher random variable is often implemented for constructing each row of circulant matrix, which forms a partial $m \times N$ random circulant matrix. Then paper \cite{rauhut2012restricted} proves that a partial random circulant matrix satisfies k order RIP when the number m of samples $m \simeq (slogN)^{3/2}$, where N is the length of the pulse. Also, the required number of samples do not reach the optimal bound in (\ref{min-sample}). In practise, these matrices are so important in signal processing applications and wireless communications where convolutions widely occur.

\subsubsection{Specific Signal Processing Matrix}
%re-wr
Many tasks signal processing do not really require full signal information but rather to solve maybe inference or estimation problems. 
In these case, fully sensing may not be sufficient so that specific sensing matrix need to designed. For instance, in \cite{davenport2010signal}, a compressive filtering matrix is introduced, which aims at sensing and directly extracting useful information without recovery. This area relates to the Compressive Signal Processing (CSP, details in chapter 5), and how to measure the minimum required number of samples is still an open issue.  

\section{Reconstruction Algorithms}\label{sct:recon-alg}

%%re-wr
In this section, we will provide a brief overview of the different types of algorithms typically used for sparse recovery. Convex optimisation algorithms require very few measurements but are computationally more complex. On the other extreme are combinatorial algorithms, which are very fast, and often sub-linear, but require many measurements that are sometimes difficult to obtain. Greedy algorithms are in some sense a good compromise between those extremes concerning computational complexity and the required number of measurements.
%%

\subsection{Convex Optimisation}\label{sct:convex}

In section \ref{sct:rip} we already stated the convex optimisation problem (P1) as (\ref{eq_l1}). If the measurements are affected by noise, a conic constraint is required and the minimisation problem needs to be changed to basis pursuit de-noising (BPDN) \cite{chen1998atomic} problem, which allows some measurement mismatch of $\epsilon > 0$:
\begin{equation}
\label{eq_bpdn}
\min \| x \|_1 \quad s.t. \quad \| y - A x \|_2 \leq \epsilon
\end{equation}
for a carefully chosen $\epsilon > 0$ . For a particular parameter $\lambda > 0$, the problem is equivalent to the unconstrained version given by
\begin{equation}
\min \| Ax - y \|_2 + \lambda \| x\|_1
\end{equation}
%xx
Developed convex optimisation algorithms specifically adapted to the compressed sensing include interior-point methods \cite{candes2006robust}, projected gradient methods \cite{figueiredo2007gradient}, and iterative thresholding \cite{blumensath2009iterative}.

\subsection{Combinatorial Algorithms}\label{sct:combin-alg}
%re-wr, xx
These methods apply group testing to highly structured samples of the original signal. From the various types of algorithms, the HHS pursuit \cite{gilbert2007one} and haining pursuit\cite{gilbert2006algorithmic} are popular algorithms, which require many measurements but achieve extremely rapid computational speed for sparse signal reconstruction. They are sensitive to noise as they strongly rely on the group testing of highly structured samples.

\subsection{Greedy Algorithms}
Greedy algorithms iteratively approximate the coefficients and the support of the original signal. They have the advantage of being very fast and easy to implement. Often the theoretical performance guarantees are very similar to, for instance, l1 minimisation results. Greedy algorithms are usually looking for the following approximate solution of the problem (P0):
\begin{equation}
min \quad \|x\|_0 \quad s.t. \quad \| Ax - y \|_2 \leq \epsilon.
\end{equation}
The basic idea is to choose the least column in A in order to form an approximate representation of y. The most commonly used as a greedy algorithm for OMP algorithm \cite{mallat1993matching}, which is a widely used for sparse reconstruction and develops the process of matching pursuit. As shown in Algorithm \ref{OMP}, it progressively manages to find the support of the unknown sparse signal: Give an acquisition system $y = Ax$ where x is K sparse vector and a $m \times N$ sensing matrix A, each iteration of OMP selects one support $\lambda$ of the vector x which contributes the most to the observation y. This selection method is based on testing the correlation values between the current columns of A and residue $r$. The OMP iteration would not stop before the residue r becomes relatively small. The OMP algorithm also has many developed versions, such as StOMP \cite{donoho2006sparse}, regularised OMP (ROMP) \cite{needell2009uniform}, and compressive sampling MP (CoSaMp) \cite{needell2009cosamp}.

\IncMargin{1em}
\begin{algorithm}
    \SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
    \SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
    \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
    \Input{measurement $A \in R^{m \times N}$, observation $y$.}
    \Output{recovery $\hat s$.}
    \BlankLine
$r_0 \gets y$; $\Lambda_0 \gets \Theta$; $i \gets 0$; \;
\While {$r_i \geq theshold$}{
$h_i \gets A^T r_i$ $\quad //match$\;
$\Lambda_{i+1} \gets \Lambda_i \cup \{\arg\max_{1 \leq j \leq N} |h_i(j)|\}$ $\quad //identity$\;
$ \hat s_{i+1} \gets \arg\min_s \|y-A_{\Lambda_{i+1}} \hat s_i\|_2$ $\quad //determine$\;
$r_{i+1} \gets y - A\hat s_{i+1}$ $\quad //update$\;
}
\caption{Orthogonal Matching Pursuit (OMP)}\label{OMP}
\end{algorithm}
\DecMargin{1em}
