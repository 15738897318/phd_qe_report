%cs-math_intro.tex
\subsection{Compressed Sensing Paradigm}\label{sct:cs_paradigm}
Compressed sensing (CS) announces that sparse signals can always be reconstructed from far fewer samples than traditional method of Nyquist Theorem. Consider a task of sampling a signal of $x \in R^N$ using sub-Nyquist rate. Suppose that a group of basis $\Psi$ provides a $K$-sparse representation of signal $x$ as following,  where $k << N$:  
\begin{equation}
\label{eq_sparse}
x=\Psi s=\sum_{l=1}^{K}\psi_l s_l  
\end{equation}
Here $x$ is a linear combination of $K$ basis chosen from $\Psi$, and $s$ is the corresponding coefficients of representing $x$ in the domain constructed by the basis $\Psi$. In CS, $x$  can always successfully be reconstructed from $M$ measurements where $M << N$. The measurements $y$ is generated by projecting $x$ over a matrix $\Phi$ incoherent with $\Psi$, which can be described as $y = \Phi x = \Phi \Psi s$. If the composite matrix $\Phi \Psi$ satisfies restricted isometry property (RIP) \cite{candes2006robust}, then s can be exactly reconstructed by solving the following $l1$-norm minimization problem (\ref{eq_l1_min}):
\begin{equation}
\label{eq_l1_min}
\hat s = \arg\min \| s \|_1 \quad s.t. \quad  y = \Phi \Psi s
\end{equation} 
It is shown in \cite{baraniuk2007compressive} that if the composite matrix $\Phi \Psi$ has all random variables taken from a normal distribution, RIP has relatively high probability to be satisfied, indicating that the $l1$-norm minimization (\ref{eq_l1_min}) can provide a stable and robust reconstruction, where only $O(K\ln(N/K))$ sampling points are needed. Therefore, CS provides a novel random sampling paradigm which requires smaller observations and releases the sampling bottleneck defined by Nyquist theory.

\subsection{Random Measurement}
Although directly designing a matrix $A = \Phi \Psi$ which satisfies the RIP is difficult, random measurement or random matrix whose entries are independent and identically distributed variables, are very likely satisfies the RIP\cite{rauhut2010compressive}. Besides, as constructing a random matrix is  applicable, Gaussian distribution matrix, Bernoulli matrix, random partial Fourier matrix, random Toeplitz matrix\cite{bajwa2007toeplitz} are widely used for constructing the measurement in CS applications. 
 
\subsection{Reconstruction Algorithms}
Solving this optimization problem (\ref{eq_l1_min}) by linear programming like basis Pursuit (BP) is computationally expensive, but recent greedy algorithms provide alternative solutions that regarded as faster and more efficient, including orthogonal matching pursuit (OMP), compressed sensing matching pursuit(CoSaMP) and iterative hard thresholding (IHT) etc. Among those sparse reconstruction algorithms, CoSaMP and IHT offer the optimal solutions as it works with a minimal number of observations and performs a better recovery robustness to noise, and requires reasonable computational complexities \cite{needell2009cosamp}. Since the matching pursuit (MP) based algorithm is the most widely used for its simple structure and low complexity, the following parts will mainly introduce two famous MP algorithms.

\subsubsection{Orthogonal Matching Pursuit}
Orthogonal matching Pursuit (OMP)\cite{mallat1993matching} is a widely used for sparse reconstruction which develops the process of matching pursuit. As shown in algorithm \ref{OMP}, it progressively manages to find the support of the unknown sparse signal: Give an acquisition system $y = As$ where $s \in R^N$ is $K$ sparse and CS measurement $A \in R^{m \times N}$ $(m << N)$, each iteration of OMP selects one support $\lambda$ of the vector $s$ which contributes the most to the observation $y$. This selection method is based on testing the correlation values between the current columns of $A$ and residue $r$. The OMP iteration would not stop before the residue $r$ becomes relatively small. 
% \begin{algorithm}[!ht]
% \caption{Orthogonal Matching Pursuit(OMP)}\label{OMP}
% \algnewcommand\algorithmicmatch{\textbf{Match:}}
% \algnewcommand\Match{\item[\algorithmicmatch]}
% \algrenewcommand{\algorithmicrequire}{\textbf{Input:}}
% \algrenewcommand{\algorithmicensure}{\textbf{Output:}}
% \begin{algorithmic}[1]
% \Require measurement $A \in R^{m \times N}$, observation $y$.
% \Ensure recovery $\hat s$.
% \State $r_0 \gets y$; $\Lambda_0 \gets \Theta$; $i \gets 0$.
% \While {$r_i \geq theshold$}
% \State $h_i \gets A^T r_i$ $\hfill (match)$ 
% \State $\Lambda_{i+1} \gets \Lambda_i \cup \{\arg\max_{1 \leq j \leq N} |h_i(j)|\}$ $\hfill (identity)$ 
% \State $ \hat s_{i+1} \gets \arg\min_s \|y-A_{\Lambda_{i+1}} \hat s_i\|_2$ $\hfill (determine)$ 
% \State $r_{i+1} \gets y - A\hat s_{i+1}$ $\hfill (update)$
% \EndWhile
% \end{algorithmic}
% \end{algorithm} 
The computational complexity of OMP is $O(KmN)$ which is significantly smaller compared to classical convex optimization such as basis pursuit whose complexity is $O(N^3)$ (in case that sparsity of $K << N$)\cite{dai2009subspace}. However, the robustness of OMP cannot reach the quality level of traditional convex optimization, since searching local optimal solutions instead of global solutions brings more errors to OMP.  

\subsubsection{Compressed Sensing Matching Pursuit}
In order to improve the speed and robustness of OMP, Compressed Sensing Matching Pursuit (CoSaMP)\cite{needell2009cosamp} is developed. It aims at producing a more effective way for detecting the support of input signals shown in \ref{CoSaMP}: The CoSaMP firstly find $2K$ indices for maximal correlation between columns of $A$ and the current residue $r$ by using the operator $H_{2K}(A^T r)$ to set all but the $2K$ largest components in $A^T r$ to zero. Then CoSaMP merges these $2K$ indices with the previous support from current recovered $\hat s$, in order to form a new support set $\lambda$ for updating the least square solution of $\hat s$. In the next step, the least squares solution is pruned and consequently only the $K$ largest components are preserved.  
% \begin{algorithm}[!ht]
% \caption{Compressed Sensing Matching Pursuit(CoSaMP)}\label{CoSaMP}
% \algnewcommand\algorithmicmatch{\textbf{Match:}}
% \algnewcommand\Match{\item[\algorithmicmatch]}
% \algrenewcommand{\algorithmicrequire}{\textbf{Input:}}
% \algrenewcommand{\algorithmicensure}{\textbf{Output:}}
% \begin{algorithmic}[1]
% \Require measurement $A \in R^{m \times N}$, observation $y$, sparsity $K$.
% \Ensure recovery $\hat s$.
% \State $r_0 \gets y$; $i \gets 0$.
% \While {$stopping \; the \; criterion$}
% \State $h_i \gets H_{2K}(A^T r_i)$  $\hfill (match)$  
% \State $\Lambda \gets supp(h_i) \cup supp(\hat s_i)$ $\hfill (identity)$
% \State $\hat s_{i+1} \gets \arg\min_s \|y-A_{\Lambda} \hat s_i\|_2$ $\hfill (determine)$
% \State $(s_{i+1})_j \gets 0 \; for \; j \not\in \Lambda_i$ 
% \State $r_{i+1} \gets y - A\hat s_{i+1}$ $\hfill (update)$ 
% \EndWhile
% \end{algorithmic}
% \end{algorithm} 

Although other revised matching pursuits such as ROMP\cite{needell2009uniform}, StOMP\cite{donoho2006sparse} have been developed for improving the robustness of OMP. However, compared to those revise for OMP, CoSaMP offers the optimal performance as it works with a minimal number of observations and performs a better recovery robustness to noise \cite{needell2009cosamp}.
